{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Opistocks - Model selection\n",
    "In this notebook we analyze multiple classifiers on multiple datasets to have the best model to perform sentiment analysis on tweets.\n",
    "\n",
    "The different classifiers tested are the following:\n",
    "- SVM linear kernel\n",
    "- SVM RBF kernel\n",
    "- Naive Bayes\n",
    "\n",
    "## Data Analysis\n",
    "There are 4 datasets of tweets, classified, by hand, in sentiment scores. The score range and nature derives from a dataset to the other. Here is a first glance of the datasets.\n",
    "- **self-drive** : 7156 tweets (213 not relevant), scores [1, 5] where 3 is neutral\n",
    "- **text-emotion** : 40k tweets, scores are 13 different emotions {empty, sadness, enthusiasm, neutral, worry, surprise, love, fun, hate, happiness, boredom, relief, anger}, we can associate empty with not relevant (827 tweets)\n",
    "- **apple** : 3886 tweets (82 not relevant), scores {1, 3 5} where 3 is neutral\n",
    "- **airline** : 14641 tweets, scores are 3 words {negative, neutral, positive}\n",
    "In total, we have (7156-213) + (40000-827) + (3886-82) + 14641 = 64561\n",
    "\n",
    "Since we have to use the less precise scale we will have 3 different classes : **positive**, **neutral**, **negative**. We will normalize the classes across all the datasets to perform the model selection. For example, for \"text-emotion\", from the 13 words, we will map them to the 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# self-drive\n",
    "df_sd = pd.read_csv('../data/Twitter-sentiment-self-drive-DFE.csv', encoding='latin1')\n",
    "\n",
    "# remove not relevant tweets\n",
    "df_sd = df_sd[~df_sd['sentiment'].isin(['not_relevant'])]\n",
    "\n",
    "# create the main dataframe by extracting the relevant rows\n",
    "df = pd.concat([df_sd['text'], df_sd['sentiment']], axis=1, keys=['text', 'sentiment'])\n",
    "\n",
    "# normalize the sentiment values ({1; 2; 3; 4; 5} => {-1; 0; 1})\n",
    "df['sentiment'] = df['sentiment'].map({'1':-1, '2':-1, '3':0, '4':1, '5':1})\n",
    "del df_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# text-emotion\n",
    "df_te = pd.read_csv('../data/text_emotion.csv')\n",
    "df_te = pd.concat([df_te['content'], df_te['sentiment']], axis=1, keys=['text', 'sentiment'])\n",
    "\n",
    "# remove not relevant tweets\n",
    "df_te = df_te[~df_te['sentiment'].isin(['empty'])]\n",
    "\n",
    "# map words into integer values ({...} => {-1; 0; 1})\n",
    "df_te['sentiment'] = df_te['sentiment'].map({\n",
    "    'sadness':-1, \n",
    "    'enthusiasm':1, \n",
    "    'neutral':0, \n",
    "    'worry':-1, \n",
    "    'surprise':1, \n",
    "    'love':1, \n",
    "    'fun':1, \n",
    "    'hate':-1, \n",
    "    'happiness':1, \n",
    "    'boredom':-1, \n",
    "    'relief':1, \n",
    "    'anger':-1})\n",
    "\n",
    "# append the rows to the main dataframe\n",
    "# df = df.append(df_te)\n",
    "del df_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# apple\n",
    "df_ap = pd.read_csv('../data/Apple-Twitter-Sentiment-DFE.csv', encoding='latin1')\n",
    "df_ap = pd.concat([df_ap['text'], df_ap['sentiment']], axis=1, keys=['text', 'sentiment'])\n",
    "\n",
    "# remove not relevant tweets\n",
    "df_ap = df_ap[~df_ap['sentiment'].isin(['not_relevant'])]\n",
    "\n",
    "# normalize the sentiment values ({1; 3; 5} => {-1; 0; 1})\n",
    "df_ap['sentiment'] = df_ap['sentiment'].map({'1':-1, '3':0, '5':1})\n",
    "\n",
    "# append the rows to the main dataframe\n",
    "# df = df.append(df_ap)\n",
    "del df_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# airline\n",
    "df_ai = pd.read_csv('../data/Airline-Sentiment-2-w-AA.csv', encoding='latin1')\n",
    "df_ai = pd.concat([df_ai['text'], df_ai['airline_sentiment']], axis=1, keys=['text', 'sentiment'])\n",
    "\n",
    "# normalize the sentiment values ({'negative'; 'neutral'; 'positive'} => {-1; 0; 1})\n",
    "df_ai['sentiment'] = df_ai['sentiment'].map({'negative':-1, 'neutral':0, 'positive':1})\n",
    "\n",
    "# append the rows to the main dataframe\n",
    "# df = df.append(df_ai)\n",
    "del df_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6943.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.159585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.602642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  6943.000000\n",
       "mean      0.159585\n",
       "std       0.602642\n",
       "min      -1.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical metrics about the classes\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    4245\n",
       " 1    1903\n",
       "-1     795\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Occurences of the different classes\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Features extraction\n",
    "Now that we have a dataframe containing all the data normalized, we can perform our model selection. In summary, we have 64560 tweets (26428 negatives, 18144 neutrals, 19988 positives) and we want to train a model based on this dataset.\n",
    "\n",
    "However, in order to train the models, we need to choose on which features we will base the classification. Here are the differents feature extraction strategies that are going to be tested:\n",
    "- Count (Bag-of-Words) VS TF-IDF\n",
    "- Stopwords from predefined list VS stopwords from dataset occurences\n",
    "- Unigram VS bigram tokenization\n",
    "- Lowercase VS not lowercase conversion\n",
    "\n",
    "There are 16 different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Array of vectorizers for the feature extraction step\n",
    "vecs = [\n",
    "    CountVectorizer(stop_words='english', ngram_range=(1, 1), lowercase=True),\n",
    "    CountVectorizer(stop_words='english', ngram_range=(1, 1), lowercase=False),\n",
    "    CountVectorizer(stop_words='english', ngram_range=(2, 2), lowercase=True),\n",
    "    CountVectorizer(stop_words='english', ngram_range=(2, 2), lowercase=False),\n",
    "    CountVectorizer(stop_words=None, ngram_range=(1, 1), lowercase=True),\n",
    "    CountVectorizer(stop_words=None, ngram_range=(1, 1), lowercase=False),\n",
    "    CountVectorizer(stop_words=None, ngram_range=(2, 2), lowercase=True),\n",
    "    CountVectorizer(stop_words=None, ngram_range=(2, 2), lowercase=False),\n",
    "    TfidfVectorizer(stop_words='english', ngram_range=(1, 1), lowercase=True),\n",
    "    TfidfVectorizer(stop_words='english', ngram_range=(1, 1), lowercase=False),\n",
    "    TfidfVectorizer(stop_words='english', ngram_range=(2, 2), lowercase=True),\n",
    "    TfidfVectorizer(stop_words='english', ngram_range=(2, 2), lowercase=False),\n",
    "    TfidfVectorizer(stop_words=None, ngram_range=(1, 1), lowercase=True),\n",
    "    TfidfVectorizer(stop_words=None, ngram_range=(1, 1), lowercase=False),\n",
    "    TfidfVectorizer(stop_words=None, ngram_range=(2, 2), lowercase=True),\n",
    "    TfidfVectorizer(stop_words=None, ngram_range=(2, 2), lowercase=False)\n",
    "]\n",
    "\n",
    "def feat(df, vec):\n",
    "    return vec.fit_transform(df['text'].as_matrix()).toarray(), df['sentiment'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Grid search model selection\n",
    "The GridSearchCV class from scikit-learn does an eshaustive search over specified parameter values for an estimator (e.g. C parameter in SVM). For every batch of features extracted by a specific vectorizer, we will split the set into distinct sets respectively the training set and the testing set (with a 75%-25% size ratio). With these two sets, the grid search find the best model and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = -1\n",
    "\n",
    "# df = df.sample(SAMPLE_SIZE)\n",
    "df['sentiment'] = df['sentiment'].map({-1:1, 0:3, 1:5})\n",
    "if SAMPLE_SIZE == -1:\n",
    "    df_sample = df\n",
    "else:\n",
    "    df_sample = df.loc[df['sentiment'] == 5].sample(SAMPLE_SIZE).append(df.loc[df['sentiment'] == 3].sample(SAMPLE_SIZE)).append(df.loc[df['sentiment'] == 1].sample(SAMPLE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "SEED = 17\n",
    "results = []\n",
    "\n",
    "for idx, vec in enumerate(vecs):\n",
    "    curvec = vec\n",
    "    X, y = curvec.fit_transform(df_sample['text'].as_matrix()).toarray(), df_sample['sentiment'].as_matrix()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED)\n",
    "#     parameters = {'kernel':('linear', 'rbf'), 'C':[0.01, 0.02, 0.05, 0.07, 0.1, 0.2, 0.5, 0.7, 1, 2, 5, 7, 10]}\n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[0.01, 0.05, 0.1, 0.5, 1, 5]}\n",
    "    svr = svm.SVC()\n",
    "    clf = GridSearchCV(svr, parameters)\n",
    "    clf.fit(X, y)\n",
    "    results.append(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "target_names = ['negative', 'neutral', 'positive']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_scores = []\n",
    "for i, r in enumerate(results):\n",
    "    im = np.argmax(r['mean_test_score'])\n",
    "    best_scores.append([np.max(r['mean_test_score']), im, r['params'][im]])\n",
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
