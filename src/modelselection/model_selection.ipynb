{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Opistocks - Model selection\n",
    "In this notebook we analyze multiple classifiers on multiple datasets to have the best model to perform sentiment analysis on tweets.\n",
    "\n",
    "The different classifiers tested are the following:\n",
    "- SVM linear kernel\n",
    "- SVM RBF kernel\n",
    "- Naive Bayes\n",
    "\n",
    "## Data Analysis\n",
    "There are 4 datasets of tweets, classified, by hand, in sentiment scores. The score range and nature derives from a dataset to the other. Here is a first glance of the datasets.\n",
    "- **self-drive** : 7156 tweets (213 not relevant), scores [1, 5] where 3 is neutral\n",
    "- **text-emotion** : 40k tweets, scores are 13 different emotions {empty, sadness, enthusiasm, neutral, worry, surprise, love, fun, hate, happiness, boredom, relief, anger}, we can associate empty with not relevant (827 tweets)\n",
    "- **apple** : 3886 tweets (82 not relevant), scores {1, 3 5} where 3 is neutral\n",
    "- **airline** : 14641 tweets, scores are 3 words {negative, neutral, positive}\n",
    "\n",
    "In total, we have (7156-213) + (40000-827) + (3886-82) + 14641 = 64561\n",
    "\n",
    "Since we have to use the less precise scale we will have 3 different classes : **positive**, **neutral**, **negative**. We will normalize the classes across all the datasets to perform the model selection. For example, for \"text-emotion\", from the 13 words, we will map them to the 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "``SEED``: the seed used in every random generator.\n",
    "\n",
    "``TRAIN_TEST_RATIO``: a number between 0 and 1 describing the percentage allowed for the testing set (e.g. 0.2 = 20% for testing set and 80% for training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TRAIN_TEST_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ============================== self-drive ===================================\n",
    "df_sd = pd.read_csv('../data/Twitter-sentiment-self-drive-DFE.csv', encoding='latin1')\n",
    "\n",
    "# remove not relevant tweets\n",
    "df_sd = df_sd[~df_sd['sentiment'].isin(['not_relevant'])]\n",
    "\n",
    "# create the main dataframe by extracting the relevant rows\n",
    "df = pd.concat([df_sd['text'], df_sd['sentiment']], axis=1, keys=['text', 'sentiment'])\n",
    "\n",
    "# normalize the sentiment values ({1; 2; 3; 4; 5} => {-1; 0; 1})\n",
    "df['sentiment'] = df['sentiment'].map({'1':-1, '2':-1, '3':0, '4':1, '5':1})\n",
    "\n",
    "# shuffle the data\n",
    "df = shuffle(df, random_state=SEED)\n",
    "\n",
    "# append the rows to the main dataframe\n",
    "df_sd = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ============================== text-emotion =================================\n",
    "df_te = pd.read_csv('../data/text_emotion.csv')\n",
    "df_te = pd.concat([df_te['content'], df_te['sentiment']], axis=1, keys=['text', 'sentiment'])\n",
    "\n",
    "# remove not relevant tweets\n",
    "df_te = df_te[~df_te['sentiment'].isin(['empty'])]\n",
    "\n",
    "# map words into integer values ({...} => {-1; 0; 1})\n",
    "df_te['sentiment'] = df_te['sentiment'].map({\n",
    "    'sadness':-1,\n",
    "    'enthusiasm':1,\n",
    "    'neutral':0,\n",
    "    'worry':-1,\n",
    "    'surprise':1,\n",
    "    'love':1,\n",
    "    'fun':1,\n",
    "    'hate':-1,\n",
    "    'happiness':1,\n",
    "    'boredom':-1,\n",
    "    'relief':1,\n",
    "    'anger':-1})\n",
    "\n",
    "# shuffle the data\n",
    "df_te = shuffle(df_te, random_state=SEED)\n",
    "\n",
    "# append the rows to the main dataframe\n",
    "df = df.append(df_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ================================= apple =====================================\n",
    "df_ap = pd.read_csv('../data/Apple-Twitter-Sentiment-DFE.csv', encoding='latin1')\n",
    "df_ap = pd.concat([df_ap['text'], df_ap['sentiment']], axis=1, keys=['text', 'sentiment'])\n",
    "\n",
    "# remove not relevant tweets\n",
    "df_ap = df_ap[~df_ap['sentiment'].isin(['not_relevant'])]\n",
    "\n",
    "# normalize the sentiment values ({1; 3; 5} => {-1; 0; 1})\n",
    "df_ap['sentiment'] = df_ap['sentiment'].map({'1':-1, '3':0, '5':1})\n",
    "\n",
    "# shuffle the data\n",
    "df_ap = shuffle(df_ap, random_state=SEED)\n",
    "\n",
    "# append the rows to the main dataframe\n",
    "df = df.append(df_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================== airline =====================================\n",
    "df_ai = pd.read_csv('../data/Airline-Sentiment-2-w-AA.csv', encoding='latin1')\n",
    "df_ai = pd.concat([df_ai['text'], df_ai['airline_sentiment']], axis=1, keys=['text', 'sentiment'])\n",
    "\n",
    "# normalize the sentiment values ({'negative'; 'neutral'; 'positive'} => {-1; 0; 1})\n",
    "df_ai['sentiment'] = df_ai['sentiment'].map({'negative':-1, 'neutral':0, 'positive':1})\n",
    "\n",
    "# shuffle the data\n",
    "df_ai = shuffle(df_ai, random_state=SEED)\n",
    "\n",
    "# append the rows to the main dataframe\n",
    "df = df.append(df_ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are some classifiers that can not be trained on negative classes (due to the implementation and/or the mathematical background). Therefore, we map the initial classes to new ones to be able to train correctly the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# map the 3 classes to another representation\n",
    "df['sentiment'] = df['sentiment'].map({-1:1, 0:3, 1:5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Having in the training set unbalanced classes might lead to performance drop. In order to avoid this risk, we extract the number of samples from the classes based on the number of elements of the smallest class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 18144 # Number of neutral occurences\n",
    "\n",
    "df = df.loc[df['sentiment'] == 5].sample(SAMPLE_SIZE).append(df.loc[df['sentiment'] == 3].sample(SAMPLE_SIZE)).append(df.loc[df['sentiment'] == 1].sample(SAMPLE_SIZE))\n",
    "df = shuffle(df, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Features extraction\n",
    "Now that we have a dataframe containing all the data normalized, we can perform our model selection. In summary, we have 64560 tweets (26428 negatives, 18144 neutrals, 19988 positives) and we want to train a model based on this dataset.\n",
    "\n",
    "However, in order to train the models, we need to choose on which features we will base the classification. Here are the differents feature extraction strategies that are going to be tested:\n",
    "- Count (Bag-of-Words) VS TF-IDF\n",
    "- Stopwords from predefined list VS stopwords from dataset occurences\n",
    "- Unigram VS bigram tokenization\n",
    "- Lowercase VS not lowercase conversion\n",
    "\n",
    "There are 16 different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Array of vectorizers for the feature extraction step\n",
    "vecs = [\n",
    "    CountVectorizer(stop_words='english', ngram_range=(1, 1), lowercase=True),\n",
    "    CountVectorizer(stop_words='english', ngram_range=(1, 1), lowercase=False),\n",
    "#     CountVectorizer(stop_words='english', ngram_range=(2, 2), lowercase=True),\n",
    "#     CountVectorizer(stop_words='english', ngram_range=(2, 2), lowercase=False),\n",
    "    CountVectorizer(stop_words=None, ngram_range=(1, 1), lowercase=True),\n",
    "    CountVectorizer(stop_words=None, ngram_range=(1, 1), lowercase=False),\n",
    "#     CountVectorizer(stop_words=None, ngram_range=(2, 2), lowercase=True),\n",
    "#     CountVectorizer(stop_words=None, ngram_range=(2, 2), lowercase=False),\n",
    "    TfidfVectorizer(stop_words='english', ngram_range=(1, 1), lowercase=True),\n",
    "    TfidfVectorizer(stop_words='english', ngram_range=(1, 1), lowercase=False),\n",
    "#     TfidfVectorizer(stop_words='english', ngram_range=(2, 2), lowercase=True),\n",
    "#     TfidfVectorizer(stop_words='english', ngram_range=(2, 2), lowercase=False),\n",
    "    TfidfVectorizer(stop_words=None, ngram_range=(1, 1), lowercase=True),\n",
    "    TfidfVectorizer(stop_words=None, ngram_range=(1, 1), lowercase=False),\n",
    "#     TfidfVectorizer(stop_words=None, ngram_range=(2, 2), lowercase=True),\n",
    "#     TfidfVectorizer(stop_words=None, ngram_range=(2, 2), lowercase=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Models training and evaluation\n",
    "Apart from the 16 preprocessing strategies, we test also different classifiers. However, since the course is more about IR and data management than machine learning techniques, we decided to train and evaluate 3 different classifiers but without changing their default parameters (i.e. parameters set by the developpers of the implementation in scikit-learn). These paramaters are the following :\n",
    "\n",
    "- SVM with Linear Kernel: ``C=1.0, loss='squared_hinge', max_iter=1000, multi_class=ovr (One-vs-Rest)``\n",
    "- SVM with RBF Kernel: ``C=1.0, degree=3, gamma='auto', multi_class=ovo (One-vs-One)``\n",
    "- NB for multinomial models: ``alpha=1.0``\n",
    "\n",
    "Special attention for the choice of the Naive Bayes model, this specific implementation is for multinomially distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dfs = [df_sd, df_te, df_ap, df_ai, df]\n",
    "dfs = [df_sd, df_ap, df_ai]\n",
    "# dfs = [df_sd[:200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== LOOP NUMBER 0 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== LOOP NUMBER 1 ========\n",
      "======== LOOP NUMBER 2 ========\n",
      "======== LOOP NUMBER 3 ========\n",
      "======== LOOP NUMBER 4 ========\n",
      "======== LOOP NUMBER 5 ========\n",
      "======== LOOP NUMBER 6 ========\n",
      "======== LOOP NUMBER 7 ========\n",
      "======== LOOP NUMBER 8 ========\n",
      "======== LOOP NUMBER 9 ========\n",
      "======== LOOP NUMBER 10 ========\n",
      "======== LOOP NUMBER 11 ========\n",
      "======== LOOP NUMBER 12 ========\n",
      "======== LOOP NUMBER 13 ========\n",
      "======== LOOP NUMBER 14 ========\n",
      "======== LOOP NUMBER 15 ========\n",
      "======== LOOP NUMBER 16 ========\n",
      "======== LOOP NUMBER 17 ========\n",
      "======== LOOP NUMBER 18 ========\n",
      "======== LOOP NUMBER 19 ========\n",
      "======== LOOP NUMBER 20 ========\n",
      "======== LOOP NUMBER 21 ========\n",
      "======== LOOP NUMBER 22 ========\n",
      "======== LOOP NUMBER 23 ========\n"
     ]
    }
   ],
   "source": [
    "results = {'linear': [], 'rbf': [], 'nb': []}\n",
    "target_names = ['negative', 'neutral', 'positive']\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    results_df_svm = []\n",
    "    results_df_rbf = []\n",
    "    results_df_nb = []\n",
    "\n",
    "    threshold = int(TRAIN_TEST_RATIO*len(dfs[i]))\n",
    "\n",
    "    for idx, vec in enumerate(vecs):\n",
    "        print('======== LOOP NUMBER {} ========'.format(i*len(vecs) + idx))\n",
    "        current_vec = vec\n",
    "        X, y = current_vec.fit_transform(dfs[i]['text'].as_matrix()).toarray(), dfs[i]['sentiment'].as_matrix()\n",
    "\n",
    "        # SVM Linear\n",
    "        svm_linear = svm.LinearSVC()\n",
    "        y_pred = svm_linear.fit(X[:threshold], y[:threshold]).predict(X[threshold:])\n",
    "        results_df_svm.append(classification_report(y[threshold:], y_pred, target_names=target_names))\n",
    "\n",
    "        # SVM RBF\n",
    "        svm_rbf = svm.SVC()\n",
    "        y_pred = svm_rbf.fit(X[:threshold], y[:threshold]).predict(X[threshold:])\n",
    "        results_df_svm.append(classification_report(y[threshold:], y_pred, target_names=target_names))\n",
    "\n",
    "        # Naive Bayes\n",
    "        gnb = MultinomialNB()\n",
    "        y_pred = gnb.fit(X[:threshold], y[:threshold]).predict(X[threshold:])\n",
    "        results_df_nb.append(classification_report(y[threshold:], y_pred, target_names=target_names))\n",
    "\n",
    "    results['linear'].append(results_df_svm)\n",
    "    results['rbf'].append(results_df_rbf)\n",
    "    results['nb'].append(results_df_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# pickle the results in order to visualize them in another notebook\n",
    "pickle.dump(results, open(\"results.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.6.0 64bit [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]"
        },
        {
         "module": "IPython",
         "version": "6.0.0"
        },
        {
         "module": "OS",
         "version": "Darwin 16.5.0 x86_64 i386 64bit"
        },
        {
         "module": "numpy",
         "version": "1.12.0"
        },
        {
         "module": "pandas",
         "version": "0.19.2"
        },
        {
         "module": "sklearn",
         "version": "0.18.1"
        },
        {
         "module": "pickle",
         "version": "The 'pickle' distribution was not found and is required by the application"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.0 64bit [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]</td></tr><tr><td>IPython</td><td>6.0.0</td></tr><tr><td>OS</td><td>Darwin 16.5.0 x86_64 i386 64bit</td></tr><tr><td>numpy</td><td>1.12.0</td></tr><tr><td>pandas</td><td>0.19.2</td></tr><tr><td>sklearn</td><td>0.18.1</td></tr><tr><td>pickle</td><td>The 'pickle' distribution was not found and is required by the application</td></tr><tr><td colspan='2'>Fri May 19 17:40:46 2017 CEST</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.6.0 64bit [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] \\\\ \\hline\n",
       "IPython & 6.0.0 \\\\ \\hline\n",
       "OS & Darwin 16.5.0 x86\\_64 i386 64bit \\\\ \\hline\n",
       "numpy & 1.12.0 \\\\ \\hline\n",
       "pandas & 0.19.2 \\\\ \\hline\n",
       "sklearn & 0.18.1 \\\\ \\hline\n",
       "pickle & The 'pickle' distribution was not found and is required by the application \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Fri May 19 17:40:46 2017 CEST} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.6.0 64bit [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]\n",
       "IPython 6.0.0\n",
       "OS Darwin 16.5.0 x86_64 i386 64bit\n",
       "numpy 1.12.0\n",
       "pandas 0.19.2\n",
       "sklearn 0.18.1\n",
       "pickle The 'pickle' distribution was not found and is required by the application\n",
       "Fri May 19 17:40:46 2017 CEST"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information about software version\n",
    "%load_ext version_information\n",
    "%reload_ext version_information\n",
    "\n",
    "%version_information numpy, pandas, sklearn, pickle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
